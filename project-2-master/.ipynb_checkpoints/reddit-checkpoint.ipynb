{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c577653-a910-400f-bcc8-ead39bc12de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install praw \n",
    "#!pip install squarify \n",
    "#!pip install emoji\n",
    "#!pip install pyfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f896c6-e068-4264-bae7-b1c9f031f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from data import *\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import squarify\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji    # removes emojis\n",
    "import re   # removes links\n",
    "import en_core_web_sm\n",
    "import string\n",
    "import yfinance as yf\n",
    "import pyfolio as pf\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def data_extractor(reddit):\n",
    "    '''extracts all the data from reddit\n",
    "    Parameter: reddt: reddit obj\n",
    "    Return:    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz\n",
    "                \n",
    "                posts: int: # of posts analyzed\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 tickers: dict: all the tickers found\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 picks: int: top picks to analyze\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                picks_ayz: int: top picks to analyze\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''############################################################################'''\n",
    "    # set the program parameters\n",
    "    subs = ['wallstreetbets','stocks' ]     # sub-reddit to search\n",
    "    post_flairs = {'Daily Discussion', 'Weekend Discussion', 'Discussion'}    # posts flairs to search || None flair is automatically considered\n",
    "    goodAuth = {'AutoModerator'}   # authors whom comments are allowed more than once\n",
    "    uniqueCmt = True                # allow one comment per author per symbol\n",
    "    ignoreAuthP = {'example'}       # authors to ignore for posts \n",
    "    ignoreAuthC = {'example'}       # authors to ignore for comment \n",
    "    upvoteRatio = 0.70         # upvote ratio for post to be considered, 0.70 = 70%\n",
    "    ups = 20       # define # of upvotes, post is considered if upvotes exceed this #\n",
    "    limit = 1     # define the limit, comments 'replace more' limit\n",
    "    upvotes = 2     # define # of upvotes, comment is considered if upvotes exceed this #\n",
    "    picks = 5     # define # of picks here, prints as \"Top ## picks are:\"\n",
    "    picks_ayz = 5   # define # of picks for sentiment analysis\n",
    "    '''############################################################################'''     \n",
    "    \n",
    "    posts, count, c_analyzed, tickers, titles, a_comments = 0, 0, 0, {}, [], {}\n",
    "    cmt_auth = {}\n",
    "    \n",
    "    for sub in subs:\n",
    "        subreddit = reddit.subreddit(sub)\n",
    "        hot_python = subreddit.hot()    # sorting posts by hot\n",
    "        # Extracting comments, symbols from subreddit\n",
    "        for submission in hot_python:\n",
    "            flair = submission.link_flair_text \n",
    "            author = submission.author.name         \n",
    "            \n",
    "            # checking: post upvote ratio # of upvotes, post flair, and author \n",
    "            if submission.upvote_ratio >= upvoteRatio and submission.ups > ups and (flair in post_flairs or flair is None) and author not in ignoreAuthP:   \n",
    "                submission.comment_sort = 'new'     \n",
    "                comments = submission.comments\n",
    "                titles.append(submission.title)\n",
    "                posts += 1\n",
    "                try: \n",
    "                    submission.comments.replace_more(limit=limit)   \n",
    "                    for comment in comments:\n",
    "                        # try except for deleted account?\n",
    "                        try: auth = comment.author.name\n",
    "                        except: pass\n",
    "                        c_analyzed += 1\n",
    "                        \n",
    "                        # checking: comment upvotes and author\n",
    "                        if comment.score > upvotes and auth not in ignoreAuthC:      \n",
    "                            split = comment.body.split(\" \")\n",
    "                            for word in split:\n",
    "                                word = word.replace(\"$\", \"\")        \n",
    "                                # upper = ticker, length of ticker <= 5, excluded words,                     \n",
    "                                if word.isupper() and len(word) <= 5 and word not in blacklist and word in us:\n",
    "                                    \n",
    "                                    # unique comments, try/except for key errors\n",
    "                                    if uniqueCmt and auth not in goodAuth:\n",
    "                                        try: \n",
    "                                            if auth in cmt_auth[word]: break\n",
    "                                        except: pass\n",
    "                                        \n",
    "                                    # counting tickers\n",
    "                                    if word in tickers:\n",
    "                                        tickers[word] += 1\n",
    "                                        a_comments[word].append(comment.body)\n",
    "                                        cmt_auth[word].append(auth)\n",
    "                                        count += 1\n",
    "                                    else:                               \n",
    "                                        tickers[word] = 1\n",
    "                                        cmt_auth[word] = [auth]\n",
    "                                        a_comments[word] = [comment.body]\n",
    "                                        count += 1   \n",
    "                except Exception as e: print(e)\n",
    "                \n",
    "                           \n",
    "    return posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec054e-c753-4331-97f0-53a131d31ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time):\n",
    "    '''prints out top tickers, and most mentioned tickers\n",
    "    \n",
    "    Parameter:   tickers: dict: all the tickers found\n",
    "                 picks: int: top picks to analyze\n",
    "                 c_analyzed: int: # of comments analyzed\n",
    "                 posts: int: # of posts analyzed\n",
    "                 subs: int: # of subreddits analyzed\n",
    "                titles: list: list of the title of posts analyzed \n",
    "                 time: time obj: top picks to analyze\n",
    "                start_time: time obj: prog start time\n",
    "\n",
    "    Return: symbols: dict: dict of sorted tickers based on mentions\n",
    "            times: list: include # of time top tickers is mentioned\n",
    "            top: list: list of top tickers\n",
    "    '''    \n",
    "\n",
    "    # sorts the dictionary\n",
    "    symbols = dict(sorted(tickers.items(), key=lambda item: item[1], reverse = True))\n",
    "    top_picks = list(symbols.keys())[0:picks]\n",
    "    time = (time.time() - start_time)\n",
    "    \n",
    "    # print top picks\n",
    "    print(\"It took {t:.2f} seconds to analyze {c} comments in {p} posts in {s} subreddits.\\n\".format(t=time, c=c_analyzed, p=posts, s=len(subs)))\n",
    "    print(\"Posts analyzed saved in titles\")\n",
    "    #for i in titles: print(i)  # prints the title of the posts analyzed\n",
    "    \n",
    "    \n",
    "    print(f\"\\n{picks} most mentioned tickers: \")\n",
    "    times = []\n",
    "    top = []\n",
    "    for i in top_picks:\n",
    "        print(f\"{i}: {symbols[i]}\")\n",
    "        times.append(symbols[i])\n",
    "        top.append(f\"{i}: {symbols[i]}\")\n",
    "   \n",
    "    return symbols, times, top\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11ad7d-bec5-4f68-8068-e64bf30d7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088228-d68e-48b5-91bc-eeddbd77db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def sentiment_analysis(picks_ayz, a_comments, symbols):\n",
    "    '''analyzes sentiment anaylsis of top tickers\n",
    "    \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 a_comments: dict: all the comments to analyze\n",
    "                 symbols: dict: dict of sorted tickers based on mentions\n",
    "    Return:      scores: dictionary: dictionary of all the sentiment analysis\n",
    "\n",
    "    '''\n",
    "    scores = {}\n",
    "     \n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    vader.lexicon.update(new_words)     # adding custom words from data.py \n",
    "    picks_sentiment = list(symbols.keys())[0:picks_ayz]\n",
    "    \n",
    "    for symbol in picks_sentiment:\n",
    "        stock_comments = a_comments[symbol]\n",
    "        for cmnt in stock_comments:\n",
    "    \n",
    "            emojiless = emoji.get_emoji_regexp().sub(u'', cmnt) # remove emojis\n",
    "            \n",
    "            # remove punctuation\n",
    "            text_punc  = \"\".join([char for char in emojiless if char not in string.punctuation])\n",
    "            text_punc = re.sub('[0-9]+', '', text_punc)\n",
    "                \n",
    "            # tokenizeing and cleaning \n",
    "            tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
    "            tokenized_string = tokenizer.tokenize(text_punc)\n",
    "            lower_tokenized = [word.lower() for word in tokenized_string] # convert to lower case\n",
    "            \n",
    "            # remove stop words\n",
    "            nlp = en_core_web_sm.load()\n",
    "            stopwords = nlp.Defaults.stop_words\n",
    "            sw_removed = [word for word in lower_tokenized if not word in stopwords]\n",
    "            \n",
    "            # normalize the words using lematization\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = ([lemmatizer.lemmatize(w) for w in sw_removed])\n",
    "            \n",
    "            # calculating sentiment of every word in comments n combining them\n",
    "            score_cmnt = {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "            \n",
    "            word_count = 0\n",
    "            for word in lemmatized_tokens:\n",
    "                if word.upper() not in us:\n",
    "                    score = vader.polarity_scores(word)\n",
    "                    word_count += 1\n",
    "                    for key, _ in score.items():\n",
    "                        score_cmnt[key] += score[key]    \n",
    "                else:\n",
    "                    score_cmnt['pos'] = 2.0               \n",
    "                    \n",
    "            # calculating avg.\n",
    "            try:        # handles: ZeroDivisionError: float division by zero\n",
    "                for key in score_cmnt:\n",
    "                    score_cmnt[key] = score_cmnt[key] / word_count\n",
    "            except: pass\n",
    "                \n",
    "            \n",
    "            # adding score the the specific symbol\n",
    "            if symbol in scores:\n",
    "                for key, _ in score_cmnt.items():\n",
    "                    scores[symbol][key] += score_cmnt[key]\n",
    "            else:\n",
    "                scores[symbol] = score_cmnt        \n",
    "    \n",
    "        # calculating avg.\n",
    "        for key in score_cmnt:\n",
    "            scores[symbol][key] = scores[symbol][key] / symbols[symbol]\n",
    "            scores[symbol][key]  = \"{pol:.3f}\".format(pol=scores[symbol][key])\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d4d90-ec37-4309-b18d-0509142f65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(scores,picks_ayz):\n",
    "    print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.index = ['Bearish', 'Neutral', 'Bullish', 'Total/Compound']\n",
    "    df = df.T\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6704c2-5025-4044-98f5-698bbf0e72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(picks_ayz, scores, picks, times, top):\n",
    "    '''prints sentiment analysis\n",
    "       makes a most mentioned picks chart\n",
    "       makes a chart of sentiment analysis of top picks\n",
    "       \n",
    "    Parameter:   picks_ayz: int: top picks to analyze\n",
    "                 scores: dictionary: dictionary of all the sentiment analysis\n",
    "                 picks: int: most mentioned picks\n",
    "                times: list: include # of time top tickers is mentioned\n",
    "                top: list: list of top tickers\n",
    "    Return:       None\n",
    "    '''\n",
    "    \n",
    "    # printing sentiment analysis \n",
    "    print(f\"\\nSentiment analysis of top {picks_ayz} picks:\")\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.index = ['Bearish', 'Neutral', 'Bullish', 'Total/Compound']\n",
    "    df = df.T\n",
    "    print(df)\n",
    "    \n",
    "    # Date Visualization\n",
    "    # most mentioned picks    \n",
    "    squarify.plot(sizes=times, label=top, alpha=.7 )\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{picks} most mentioned picks\")\n",
    "    #plt.show()\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    df = df.astype(float)\n",
    "    colors = ['red', 'springgreen', 'forestgreen', 'coral']\n",
    "    df.plot(kind = 'bar', color=colors, title=f\"Sentiment analysis of top {picks_ayz} picks:\")\n",
    "    \n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e5f64-5871-4bb5-a2d7-ee7c09eb23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMA(data,period=30,column='Close'):\n",
    "    return data[column].rolling(window=period).mean()\n",
    "def EMA(data,period=20,column='Close'):\n",
    "    return data[column].ewm(span=period,adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcb521-89ad-4ced-b11c-f9d625fcdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StockPrice(symbols):\n",
    "    '''prints data frame  \n",
    "    Parameter:   symbols\n",
    "    Return:       data frame for top 5 sentiment analys stock price \n",
    "    '''\n",
    "    today_date = date.today()\n",
    "    three_Month = str(today_date -  timedelta(days=90))\n",
    "    period_long =26\n",
    "    period_short = 12\n",
    "    period_signal = 9\n",
    "    month_period = 30\n",
    "    sma_period = 30\n",
    "    #five = symbols.item()\n",
    "    top_5 = list(symbols.keys())[0:5]\n",
    "    df_price = pd.DataFrame()\n",
    "    \n",
    "    for i in top_5:\n",
    "        i_data = yf.download(tickers= i, start=three_Month, end=today_date,interval='1d')\n",
    "        i_out_data = i_data.drop(columns = ['Open','High','Low','Adj Close','Volume'])\n",
    "        i_out_data.rename(columns={'Close':i},inplace=True)\n",
    "        \n",
    "        delta = i_out_data[i].diff(1)\n",
    "        delta = delta[1:]\n",
    "        up = delta.copy()\n",
    "        down = delta.copy()\n",
    "        up[up<0] = 0\n",
    "        down[down>0]=0\n",
    "        i_out_data['UP'] = up\n",
    "        i_out_data['down'] = down\n",
    "        AVG_Gain = SMA(i_out_data,month_period,column = 'UP')\n",
    "        AVG_Loss = abs(SMA(i_out_data,month_period,column = 'down'))\n",
    "        RS = AVG_Gain / AVG_Loss\n",
    "        RSI = 100.0 - (100.0/(1.0 + RS))\n",
    "        i_out_data['RSI'] =  RSI\n",
    "        i_out_data = i_out_data.drop(columns = ['UP','down'])\n",
    "        ShortEMA = EMA(i_out_data,period_short,column=i)\n",
    "        LongEMA = EMA(i_out_data,period_long,column = i)\n",
    "        i_out_data['MACD'] = ShortEMA - LongEMA\n",
    "        i_out_data['Signal_Line'] = EMA(i_out_data, period_signal ,column ='MACD')\n",
    "        i_out_data['SMA_30'] = SMA(i_out_data,sma_period,column=i)\n",
    "        std = i_out_data['SMA_30'].rolling(window=30).std()\n",
    "        i_out_data['Upper_Band'] = i_out_data['SMA_30']+std*2\n",
    "        i_out_data['lower_Band'] = i_out_data['SMA_30']-std*2\n",
    "        i_out_data['Prediction_close'] = i_out_data[[i]].shift(+1)\n",
    "        i_out_data['Pct_change'] = i_out_data[[i]].pct_change(+1)       \n",
    "        #print(i_out_data.head())\n",
    "        df_price = pd.concat([i_out_data,df_price], axis='columns', join='outer', ignore_index = False)\n",
    "        df_price.dropna(inplace = True)\n",
    "        print(df_price.head())\n",
    "        \n",
    "        X = df_price.drop(columns=['Prediction_close','Pct_change'])\n",
    "        print(X)\n",
    "        y = df_price['Prediction_close'].copy()\n",
    "    \n",
    "        X_lr = df_price.drop(columns=['Prediction_close','Pct_change'])\n",
    "        y_lr = df_price['Pct_change'].copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        lenght_lr = int(round(len(X)*.5,0))\n",
    "        X_train_lr = X[:lenght_lr]\n",
    "        X_test_lr = X[lenght_lr:]\n",
    "        y_train_lr =y[:lenght_lr]\n",
    "        y_test_lr = y[lenght_lr:]\n",
    "        \n",
    "        length = int(round(len(X)*.5,0))\n",
    "        X_train = X[:length]\n",
    "        X_test = X[length:]\n",
    "        y_train = y[:length]\n",
    "        y_test = y[length:]\n",
    "\n",
    "        lr = LinearRegression().fit(X_train_lr, y_train_lr)\n",
    "        prediction_lr = lr.predict(X_test_lr)\n",
    "        results_df_lr = pd.DataFrame({'Real':y_test_lr, 'Predictions':prediction_lr})\n",
    "        results_df_lr['Real Buy/Sell'] = np.where(results_df_lr['Real']>=0,'Sell','Buy')\n",
    "        results_df_lr['Predict Buy/Sell'] = np.where(results_df_lr['Predictions']>=0,'Sell','Buy')\n",
    "        results_df_lr['Model test'] = np.where(results_df_lr['Real Buy/Sell']==results_df_lr['Predict Buy/Sell'],1,0)\n",
    "        \n",
    "        print(f\"Linear result :{results_df_lr}\")\n",
    "        print(f\"Linear prediction :{prediction_lr[:5]}\")\n",
    "        print(f\"linear result :{y_test_lr[:5]}\")\n",
    "        #plt.scatter(X_test_lr, y_test_lr)\n",
    "        #plt.plot(X_test_lr, prediction_lr, color='red')\n",
    "        \n",
    "        score_lr = lr.score(X_lr,y_lr)\n",
    "        r2_lr = r2_score(y_test_lr,prediction_lr)\n",
    "        mse_lr = mean_squared_error(y_test_lr,prediction_lr)\n",
    "        print(f\"Linear regression score:{score_lr}, r2_lr:{r2_lr}, mse_lr={mse_lr}\")\n",
    "        \n",
    "        \n",
    "        tree = DecisionTreeRegressor().fit(X_train,y_train)\n",
    "        prediction = tree.predict(X_test)\n",
    "        results_df = pd.DataFrame({'Real':y_test, 'Predictions':prediction})\n",
    "        results_df['Real Buy/Sell'] = np.where(results_df['Real']>=0,'Sell','Buy')\n",
    "        results_df['Predict Buy/Sell'] = np.where(results_df['Predictions']>=0,'Sell','Buy')\n",
    "        results_df['Model test'] = np.where(results_df['Real Buy/Sell']==results_df['Predict Buy/Sell'],1,0)\n",
    "        \n",
    "        print(f\"Decision Tree :{results_df}\")        \n",
    "        print(f\"Decision Tree prediction : {prediction[:5]}\")\n",
    "        print(f\"Decision Tree result :{y_test[:5]}\")\n",
    "        \n",
    "        score = tree.score(X ,y)\n",
    "        r2 = r2_score(y_test, prediction)\n",
    "        mse = mean_squared_error(y_test, prediction)\n",
    "        print(f\"Desicon Tree :{score}, r2:{r2}, mse = {mse}\")\n",
    "        \n",
    "        #df[i] = list(i_out_data)\n",
    "        #df.append(i_out_data)\n",
    "        #df_price.dropna(inplace = True)\n",
    "        df_price = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2537814-d432-4dbb-967b-161694fd9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#StockPrice(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6919a5-ac32-44f0-b6b2-7d9f18df3138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''main function\n",
    "    Parameter:   None\n",
    "    Return:       None\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # reddit client\n",
    "    reddit = praw.Reddit(user_agent=\"Comment Extraction\",\n",
    "                         client_id=\"VpHhcn4Pt6u42DfPBnbOrQ\",\n",
    "                         client_secret=\"YOFjc2YV0z3vxx0dU_eAT55VjprgJQ\",\n",
    "                         username=\"ayushshah1204\",\n",
    "                         password=\"Ayush@1204\")\n",
    "\n",
    "    posts, c_analyzed, tickers, titles, a_comments, picks, subs, picks_ayz = data_extractor(reddit)\n",
    "    symbols, times, top = print_helper(tickers, picks, c_analyzed, posts, subs, titles, time, start_time)\n",
    "    scores = sentiment_analysis(picks_ayz, a_comments, symbols)\n",
    "    stock_price = StockPrice(symbols)\n",
    "    sentiment(scores,picks_ayz)\n",
    "    visualization(picks_ayz, scores, picks, times, top)\n",
    "    return stock_price\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   df =  main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b7a55-1c29-481a-908a-5c91aae5c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put them in a datafame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b280bf-cc24-4ed6-92f5-267319863c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
